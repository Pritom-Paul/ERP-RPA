# PDF Extraction Code - Technical Documentation

## Extraction Architecture Overview

**Implementation**: The script employs a dual text extraction approach combining **pdfplumber** for general content extraction and **PyPDF** for layout-sensitive data processing. The system processes PDF files through a multi-stage pipeline: initial text extraction → header/footer cleaning → field extraction using regex patterns → table parsing via Camelot → size/quantity mapping through PyPDF → data validation → Excel output generation.

**Data Processing Pipeline**: 
1. Directory scanning for PDF files
2. Parallel text extraction (pdfplumber for content, PyPDF for layout)
3. Page-level header/footer removal using regex patterns
4. Field extraction through compiled regex patterns
5. Table extraction using Camelot stream flavor
6. Size/quantity alignment using column position analysis
7. Data validation with length matching requirements
8. DataFrame construction and Excel export

---

## Page Processing

### Text Extraction Implementation

**Implementation**: The script uses `pdfplumber.open()` to extract raw text from each page via `page.extract_text()`, accumulating content in the `all_text` variable. Simultaneously, it processes each page through a header/footer cleaning mechanism.

**Example Input/Output**:
- **Input**: PDF page containing "*05B00041981*\nOrder 12345\nPage 1 of 3\nActual content..."
- **Output**: `all_text` contains "Actual content..." with headers removed

**Handling Notes**: The script maintains two text streams: `all_text` for complete raw content and `all_pages_body` for cleaned page content. The `header_pattern = re.compile(r'^\*\d{2}B\d+\*$')` identifies headers like "*05B00041981*".

### Header and Footer Removal Logic

**Implementation**: The script processes each page line-by-line using state tracking via the `in_header` boolean flag. Lines matching the header pattern trigger header mode, which continues until encountering a line starting with "Order ".

**Handling Notes**: 
- Lines matching `r'^Page \d+ of \d+$'` are completely filtered out
- Header detection uses the pattern `^\*\d{2}B\d+\*$`
- Header mode terminates when encountering lines starting with "Order "
- The `page_body_lines` list accumulates cleaned content per page

---

## Size and Quantity Extraction (PyPDF Implementation)

### Layout-Preserved Text Extraction

**Implementation**: The `extract_pdf_text_preserve_layout()` function uses `PdfReader` from PyPDF with `extraction_mode="layout"` and `keep_blank_chars=True` parameters to maintain spatial positioning of text elements.

**Example Input/Output**:
- **Input**: PDF with columnar size/quantity data
- **Output**: `pages_text` dictionary mapping page numbers to layout-preserved text strings

### Token Column Position Analysis

**Implementation**: The `tokens_with_cols()` function uses `re.finditer(r"\S+", line)` to extract non-whitespace tokens along with their starting column positions via `m.start()`.

**Example Input/Output**:
- **Input**: "Size    XS    S     M     L"
- **Output**: `[('Size', 0), ('XS', 8), ('S', 14), ('M', 20), ('L', 26)]`

### Size-Quantity Mapping Algorithm

**Implementation**: The `extract_size_qty_mappings()` function uses the regex pattern `r"(Size[^\n]+)\n\s*(Qty[^\n]+)"` to identify size-quantity table pairs. It implements two mapping strategies:

1. **Direct Mapping** (when `len(size_tokens) == len(qty_tokens)`): Direct index-based pairing
2. **Column Alignment** (when lengths differ): Spatial positioning using column ranges

**Handling Notes**: 
- Quantity values undergo normalization: `norm = q_tok.replace('.', '').replace(',', '')`
- Non-numeric quantities default to 0
- Column ranges calculated as midpoints between adjacent size positions
- The function returns a list of dictionaries mapping sizes to quantities

---

## Field Extraction Implementation

### Order Header Fields

**Implementation**: All header fields use `re.search()` with specific patterns and capture groups:

| Field | Regex Pattern | Capture Group | Fallback |
|-------|---------------|---------------|----------|
| **Order Number** | `r"Order\s*([A-Za-z0-9]+)"` | Group 1 | "Not found" |
| **Buy-from Vendor No** | `r"Buy-from Vendor No\.?\s*([0-9]+)"` | Group 1 | "Not found" |
| **Order Date** | `r"Order Date\s*([0-9]{2}-[0-9]{2}-[0-9]{4})"` | Group 1 | "Not found" |
| **Purchaser** | `r"Purchaser\s*([^\n\d]+)"` | Group 1 (stripped) | "Not found" |

**Example Input/Output**:
- **Input**: "Order ABC123 Buy-from Vendor No. 456789"
- **Output**: `order_number = "ABC123"`, `buy_from_vendor_no = "456789"`

### Email Field Special Processing

**Implementation**: Email extraction uses a two-step process:
1. Pattern: `r"E-Mail\s*([^\n@]+@)"` captures prefix before @ symbol
2. Post-processing: Removes trailing '@' and appends "@brands-fashion.com"

**Example Input/Output**:
- **Input**: "E-Mail john.doe@"
- **Output**: `email = "john.doe@brands-fashion.com"`

**Handling Notes**: If no match found, defaults to "Not found"

### Numeric Field Processing with Locale Support

**Implementation**: Total pieces and Total USD use `parse_decimal()` with `locale='de_DE'` for European number format conversion:

| Field | Pattern | Processing |
|-------|---------|------------|
| **Total Pieces** | `r"Total PIECES\s*([\d\.,]+)"` | `parse_decimal(match, locale='de_DE')` |
| **Total USD** | `r"Total USD\s*([\d\.,]+)"` | `parse_decimal(match, locale='de_DE')` |

**Example Input/Output**:
- **Input**: "Total USD 1.234,56"
- **Output**: `total_usd = "1234.56"`

---

## Table Parsing (Camelot Implementation)

### Table Extraction Configuration

**Implementation**: The script uses `camelot.read_pdf(pdf_path, pages='all', flavor='stream')` to extract all tables across all pages, storing results in `camelot_tables` as a list of DataFrames.

### Address Extraction Logic

**Implementation**: The script implements a two-tier search strategy for address data:

#### Primary Search Strategy
**Implementation**: Iterates through `camelot_tables` checking if first cell contains "Ship-to Address". Upon match:
- `ship_to_address = df.iloc[1, 0].strip()`
- `agency_to_address = df.iloc[1, 1].strip()`
- Buying house address concatenated from rows 2+ in column 0
- Agency full address concatenated from rows 2+ in column 1

#### Fallback Search Strategy
**Implementation**: If `found_address` remains False, performs exhaustive cell-by-cell search across all tables for "Ship-to Address" text.

**Handling Notes**: The `found_address` flag prevents redundant processing. All address fields default to "Not found" if no matches discovered.

---

## Order Details Processing

### Content Capture Logic

**Implementation**: The script uses state-based line capture controlled by the `capture` boolean flag with multiple trigger conditions:

| Trigger Condition | Action |
|-------------------|--------|
| "This document contains certified products..." | Set `capture = True` |
| "shipment FOB date" (if first trigger not found) | Set `capture = True` |
| "Applicable Certifications" | Set `capture = False` |
| Beyond last "Total USD" index | Set `capture = False` |

**Handling Notes**: The `last_total_usd_index` provides a fallback termination point when "Applicable Certifications" text is absent.

### Order Block Parsing

**Implementation**: Order blocks extracted using regex pattern `r'^(.*?Amount\s[\d\.,]+\s*\w*)'` with `re.DOTALL | re.MULTILINE` flags to capture multi-line content ending at "Amount" lines.

**Example Input/Output**:
- **Input**: Multi-line order block ending with "Amount 1,234.56 USD"
- **Output**: Complete text block captured for individual field extraction

---

## Order Block Field Extraction

### Style and Identification Fields

**Implementation**: Each order block undergoes systematic field extraction:

| Field | Extraction Method | Processing |
|-------|------------------|------------|
| **Style No** | `lines[0].split()[0]` | First word of first line |
| **Amount** | `r'Amount\s([\d\.,]+)'` | Decimal parsing with German locale |
| **Color & Customs No** | `r'([^\n]*?)\bCustoms\s+no\.?\s*:\s*([\d ]+)'` | Split extraction from same line |

### Date and Pricing Fields

**Implementation**: 
- **Shipment FOB Date**: Pattern `r'shipment\s+FOB\s+date\s+(\d{1,2}\.\s+\w+\s+\d{4})'`
- **Price**: Pattern `r'Price\s*:?\s*([\d\.,]+)'` with German locale decimal parsing
- **Total Quantity**: Pattern `r'Total\s([\d\.,]+)'` with German locale decimal parsing

**Example Input/Output**:
- **Input**: "shipment FOB date 15. March 2024"
- **Output**: `shipment_fob_date = "15. March 2024"`

### Style Description Construction

**Implementation**: Multi-step description assembly:
1. Extract first line, remove style number, truncate at "shipment FOB date"
2. Append subsequent lines until "Customs no" encountered
3. Join all parts with spaces, filtering empty strings

**Handling Notes**: Uses regex `r'(.*?)\bshipment\s+FOB\s+date'` for first line truncation with case-insensitive matching.

---

## Certification Handling

### Certification Extraction

**Implementation**: The script uses `re.findall(r"Certifications:\s*(.+)", all_text)` to extract all certification lines, storing results in `applicable_certifications` list after stripping whitespace.

### Certification-Style Mapping

**Implementation**: Creates `cert_map` dictionary using pattern `r'([^\n]+)\nCertifications:\s*(.+)'` to associate style descriptions with certifications.

**Handling Notes**: 
- Length comparison between `applicable_certifications` and `order_blocks` determines mapping strategy
- If lengths match: index-based assignment
- If lengths differ: dictionary-based lookup using style descriptions

---

## Data Validation and Error Handling

### Length Validation Checks

**Implementation**: The script implements multiple length validation points:

1. **Size/Quantity Lists**: `if len(sizes_list) != len(quantities_list): raise ValueError("Sizes and Quantities lists have different lengths!")`
2. **DataFrame/Lists Alignment**: `if len(orders_df) != len(sizes_list) or len(orders_df) != len(quantities_list): raise ValueError(...)`

**Handling Notes**: Both validations use `raise ValueError()` to terminate execution on mismatches, preventing data corruption.

### Fallback Value Assignment

**Implementation**: All extracted fields implement consistent fallback logic:
- Regex matches: `field = match.group(1) if match else "Not found"`
- Numeric conversions: Try `parse_decimal()`, fallback to "Not found"
- Address extraction: Default initialization to "Not found", conditional updates

---

## DataFrame Construction and Output

### Data Structure Assembly

**Implementation**: The script creates one DataFrame row per size variant, populating fixed fields identically across all rows for each order block. The loop `for i in range(len(sizes)):` generates the required number of rows.

### Size/Quantity Assignment

**Implementation**: After DataFrame initialization with `None` values for Size/Qty columns:
```python
orders_df['Size'] = sizes_list
orders_df['Qty'] = quantities_list
```

**Handling Notes**: This assignment occurs after length validation ensures matching list sizes.

### Excel Export Implementation

**Implementation**: Uses `orders_df.to_excel(output_excel, index=False)` with filename constructed as `f"{os.path.splitext(filename)[0]}_orders.xlsx"`.

**Handling Notes**: Output files saved in same directory as source PDFs with "_orders.xlsx" suffix. No error handling implemented for file write operations.

---

## Error Conditions and Failure Points

### Critical Failure Points

1. **Length Mismatches**: Script terminates with `ValueError` if sizes/quantities lists have different lengths
2. **File Access**: No error handling for PDF read failures or Excel write permissions
3. **Regex Failures**: Silent fallback to "Not found" for all field extractions
4. **Table Parsing**: Camelot failures result in empty table list, addresses default to "Not found"

### Data Quality Issues

**Implementation**: The script provides no validation for:
- Extracted field format correctness
- Date format validation beyond regex patterns  
- Numeric field range checking
- Address completeness verification

**Handling Notes**: All field extractions use permissive patterns with graceful degradation to "Not found" values rather than validation failures.